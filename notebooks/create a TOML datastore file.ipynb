{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bdcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook shows how to take some shape information, format it to suit the echoSMs\n",
    "# anatomical datastore format, add metadata, and write out in TOML format.abs\n",
    "\n",
    "# This code can be a starting point for writing your own Python code to convert your shape\n",
    "# data into anatomical datastore TOML files.\n",
    "\n",
    "# This demo code only creates one dataset with one specimen containing one shape.\n",
    "# You may find it useful to generalise it to create multiple datassets, specimens,\n",
    "# and shapes.\n",
    "\n",
    "import tomli_w\n",
    "import requests\n",
    "import copy\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "\n",
    "worms_url = 'https://www.marinespecies.org/rest/'\n",
    "\n",
    "# Datset. Refer to the schema documentation for explanations of these, including\n",
    "# data types, allowed values, and which are mandatory.\n",
    "dataset = {'dataset_id': 'demo_123456',  # needs to be unique across all datasets\n",
    "           'description': \"\",\n",
    "           'anatomical_category': \"organism\",\n",
    "           'date_first_added': date.today().strftime('%Y-%m-%d'),  # a placeholder date\n",
    "           'date_last_modified': date.today().strftime('%Y-%m-%d'),  # a placeholder date\n",
    "           'aphiaID': 126436,  # visit www.marinespecies.org and find the id for your species\n",
    "           'class': \"\",\n",
    "           'order': \"\",\n",
    "           'family': \"\",\n",
    "           'genus': \"\",\n",
    "           'species': \"\",\n",
    "           'vernacular_name': \"\",\n",
    "           'reference': '',\n",
    "           # 'activity_name': \"\",\n",
    "           # 'location': \"\",\n",
    "           # 'latitude': np.nan,\n",
    "           # 'latitude_units': 'degrees_north',\n",
    "           # 'longitude': np.nan,\n",
    "           # 'longitude_units': 'degrees_east',\n",
    "           # 'depth': np.nan,\n",
    "           # 'depth_units': 'm',\n",
    "           'date_collection': \"\",\n",
    "           'date_image': \"\",\n",
    "           'investigators': [],\n",
    "           'data_collection_description': \"\",\n",
    "           'note': \"\",\n",
    "           'imaging_method': \"unknown\",\n",
    "           'shape_method': \"unknown\",\n",
    "           'shape_method_processing': \"unknown\",\n",
    "           'model_type': \"\",\n",
    "           'sound_speed_method': \"unknown\",\n",
    "           'mass_density_method': \"unknown\",\n",
    "           'dataset_size': 0.0,  # the datatore loader program will overwrite this\n",
    "           'dataset_size_units': 'megabyte',\n",
    "           'specimens': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query WoRMS to get the full species classification naming using the aphia ID.\n",
    "#  This is just a convenience as you could provide these attributes manually\n",
    "\n",
    "r = requests.get(worms_url + 'AphiaRecordByAphiaID/' + str(dataset['aphiaID']))\n",
    "if r.status_code != 200:\n",
    "    print('Failed to get record for aphiaID of {dataset[\"aphiaID\"]}')\n",
    "aphia_data = r.json()\n",
    "\n",
    "# and populate the relevant echoSMs anatomical datastore metadata fields\n",
    "for attr in ['class', 'order', 'family', 'genus']:\n",
    "    dataset[attr] = aphia_data[attr]\n",
    "dataset['species'] = aphia_data['scientificname']\n",
    "\n",
    "# Get a vernacular name from WoRMS too\n",
    "r = requests.get(worms_url + 'AphiaVernacularsByAphiaID/' + str(dataset['aphiaID']))\n",
    "# If there are some for this species, choose one\n",
    "if r.status_code == 200 and len(vers := r.json()) > 0:\n",
    "    dataset['vernacular_name'] = vers[0]['vernacular']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dataset can have one or more specimens\n",
    "specimen = {'specimen_id': 'some specimen name',\n",
    "            'specimen_condition': 'unknown',\n",
    "            'length': 0.45,\n",
    "            'length_units': 'm',\n",
    "            # 'weight': 0.0,\n",
    "            # 'weight_units': 'kg',\n",
    "            'sex': 'unknown',\n",
    "            'length_type': 'unknown',\n",
    "            'shape_type': 'outline',\n",
    "            'shapes': []}\n",
    "\n",
    "# Each specimen can have one or more shapes\n",
    "shape = {'shape_units': 'm',\n",
    "         'boundary': 'pressure-release',\n",
    "         'x': [],\n",
    "         'y': [],\n",
    "         'z': [],\n",
    "         'height': [],\n",
    "         'width': [],\n",
    "         'mass_density': [],\n",
    "         'mass_density_units': 'kg/m^3',\n",
    "         'sound_speed_compressional': [],\n",
    "         'sound_speed_compressional_units': 'm/s'}\n",
    "\n",
    "# Add the shape into the specimen\n",
    "specimen['shapes'].append(shape)\n",
    "\n",
    "# Add the specimen into the dataset\n",
    "dataset['specimens'].append(specimen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76209d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the entire dataset to a TOML file\n",
    "\n",
    "# It might also be sensible to do the validatation against the schema here - the \n",
    "# datastore loading process does validation, but it's probably better to know about\n",
    "# validation errors now rather than later.\n",
    "\n",
    "# Convert any arrays to lists (to facilitate exporting to toml)\n",
    "ds = copy.deepcopy(dataset)\n",
    "for ss in ds['specimens']:\n",
    "    for s in ss['shapes']:\n",
    "        for k in s.keys():\n",
    "            if isinstance(s[k], np.ndarray):\n",
    "                s[k] = s[k].tolist()\n",
    "\n",
    "# As per the echoSMs datatore guidelines, the TOML file should be placed in \n",
    "# a directory with the same name as the dataset_id\n",
    "dataset_file = Path.home()/'datasets'/dataset['dataset_id']/'metadata.toml'\n",
    "\n",
    "# Create the directory for the TOML file\n",
    "Path.mkdir(dataset_file.parent, parents=True, exist_ok=True)\n",
    "\n",
    "# Write the TOML file\n",
    "with open(dataset_file, 'wb') as f:\n",
    "    tomli_w.dump(ds, f)\n",
    "print('Wrote TOML file to', dataset_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
